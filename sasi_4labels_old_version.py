# -*- coding: utf-8 -*-
"""SASI_4labels_old_version

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10Q3ASN_wPHQD3PyrXAkzqfOAZe2EBYHG

## 2.1. Word Embedding Construction
*related to the section 4.1 and 4.3*
"""

from google.colab import drive
drive.mount('/content/drive/')
!pip install transformers
!pip install -U sentence-transformers

from transformers import AutoTokenizer, AutoModelForSequenceClassification,AutoModel
from sentence_transformers import SentenceTransformer
from transformers import LongformerModel, LongformerTokenizer
from sentence_transformers import SentenceTransformer
from transformers import pipeline
from transformers import BertTokenizer, BertModel
import pandas as pd
import numpy as np
import torch

# open
import pickle
'''with open ('/content/drive/MyDrive/CS40-2/CS40-2 Data Collections/CS40-2 data/W9/d6290_allposts.pkl', 'rb') as f1:
    data6290 = pickle.load(f1)'''

with open ('/content/drive/MyDrive/CS40-2/CS40-2 Data Collections/CS40-2 data/W11/clean__D170_4class.pkl', 'rb') as f2:
    data170 = pickle.load(f2)

with open ('/content/drive/MyDrive/CS40-2/CS40-2 Data Collections/CS40-2 data/W11/clean__D3_4class.pkl', 'rb') as f3:
    data3553 = pickle.load(f3)

# /content/drive/MyDrive/CS40-2/CS40-2 Data Collections/CS40-2 data/W11/reddit-longformer.pkl
import pickle

with open ('/content/drive/MyDrive/CS40-2/CS40-2 Data Collections/CS40-2 data/W11/reddit-longformer.pkl', 'rb') as f4:
    redditdata = pickle.load(f4)
redditdata

len(redditdata['enc'][0][0])

#text6290 = data6290['texts'].tolist()
#3label6290 = data6290['label'].tolist()
#label6290 = torch.from_numpy(np.array(label6290))

text170 = data170['text'].tolist()
label170 = data170['label'].tolist()
label170 = torch.from_numpy(np.array(label170))

text3553 = data3553['text'].tolist()
label3553 = data3553['label'].tolist()
label3553 = torch.from_numpy(np.array(label3553))

textr = redditdata['Post'].tolist()
labelr = redditdata['label'].tolist()
labelr = torch.from_numpy(np.array(labelr))

def flatten(t):
    return [item for sublist in t for item in sublist]

textr = flatten(textr)

len(textr)

"""## 2.2. Pretrained Word Embedding
*related to the section 4.3*
"""

# model_name = "ncduy/bert-base-cased-finetuned-emotion"
# model_name = 'bert-base-uncased'
# model_name = 'allenai/longformer-base-4096'
# model_name = 'sentence-transformers/all-MiniLM-L6-v2'

## bert
bert_model = BertModel.from_pretrained('bert-base-uncased')
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')


## longformer
# bert_model = LongformerModel.from_pretrained('allenai/longformer-base-4096', gradient_checkpointing=True, attention_window=[256]*12)
# tokenizer = LongformerTokenizer.from_pretrained('allenai/longformer-base-4096')

## sbert
# bert_model =  AutoModel.from_pretrained('sentence-transformers/all-mpnet-base-v2')
# tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-mpnet-base-v2')

result_id_170 = tokenizer(text170,padding=True,truncation=True,return_tensors='pt',max_length=500)
#result_id_6290 = tokenizer(text6290,padding=True,truncation=True,return_tensors='pt',max_length=100)
result_id_3553 = tokenizer(text3553,padding=True,truncation=True,return_tensors='pt',max_length=500)

## 截取长度为500

bert_model = BertModel.from_pretrained('bert-base-uncased')
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

result_id_reddit = tokenizer(textr,padding=True,truncation=True,return_tensors='pt',max_length=500)

import numpy as np
import pandas as pd

x170 = result_id_170['input_ids']
y170 = label170

#x6290 = result_id_6290['input_ids']
#y6290 = label6290

x3553 = result_id_3553['input_ids']
y3553 = label3553

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

import numpy as np
import pandas as pd

xr = result_id_reddit['input_ids']
yr = labelr

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

yr.shape

"""# 3 - Model Implementation

### 3.1. Build Sequence Model (Bi-directional model)
*related to the section 4.4*

### Attention-Based BiLSTM + MLP with Softmax
reference: https://aclanthology.org/P16-2034.pdf
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from sklearn.metrics import accuracy_score
from sklearn.metrics import classification_report
import gc

# Define the model
class LR(nn.Module):
  def __init__(self):
    super(LR,self).__init__()
    self.fc=nn.Linear(16,2)
  def forward(self,x):
    out=self.fc(x)
    out=torch.sigmoid(out)
    return out

class mlp(nn.Module):
  def __init__(self, input_dim, hidden_dim, output_dim):
    super(mlp, self).__init__()
    self.fc1 = nn.Linear(input_dim, hidden_dim)
    self.fc2 = nn.Linear(hidden_dim, output_dim)
  
  def forward(self, x_in, apply_softmax=True):
    intermediate = F.relu(self.fc1(x_in))
    output = self.fc2(intermediate)
    if apply_softmax:
      output = F.softmax(output, dim=1)
    return output

class bilstmAttnMLP(nn.Module):
  def __init__(self, input_size, num_classes, hidden_size, hidden_size2, n_layers, pretrain_embedding, bidirectional=True):
    super(bilstmAttnMLP,self).__init__()
    self.embedding = pretrain_embedding
    for param in self.embedding.parameters():
      param.requires_grad = False
    
    self.lstm = nn.LSTM(input_size, hidden_size, n_layers, batch_first=True, bidirectional=bidirectional)

    self.tanh1 = nn.Tanh()
    self.w = nn.Parameter(torch.zeros(hidden_size * 2))
    # self.tanh2 = nn.Tanh()

    self.fc1 = nn.Linear(hidden_size * 2, hidden_size2)
    self.fc = nn.Linear(hidden_size2, num_classes)

    #lr
    #self.fc_lr = nn.Linear(num_classes,num_classes)
    #self.sm = nn.Sigmoid()
    self.softm = nn.Softmax(dim=1)
    # mlp

  def forward(self,x):
    # bilstm+attn
    emb = self.embedding(x)[0]
    H, _ = self.lstm(emb)
    M = self.tanh1(H)
    alpha = F.softmax(torch.matmul(M, self.w), dim=1).unsqueeze(-1)
    # hstar = self.tanh2(r)
    out = torch.sum(H * alpha, 1)

    # mlp
    out = F.relu(out)
    out = self.fc1(out)
    out = self.fc(out)
    out = self.softm(out)
    
    # lr
    
    g = F.sigmoid(out)
    #g=self.sm(g)
    #print(g)


    # mlp
    # out = F.relu(self.fc1(out))
    # out = self.fc(out)
    # out = F.softmax(out, dim=1)
    # g = torch.softmax(r, dim=1) [16,2]
    return out,g

"""### Gambler's Loss Function Define
reference: https://zhuanlan.zhihu.com/p/77964128
"""

import numpy as np
import math

class GamblerLoss():

  def __init__(self, weight=None, size_average=False): # label,predicted,reward,gain

    self.weight = weight
    self.size_average = size_average

  def __call__(self,label,predicted,reward,gain):

    batch_loss = 0.

    for i in range(predicted.shape[0]):
      if label[i] == 0:
        loss = -torch.log(predicted[i][0] * reward + gain[i])
      elif label[i] == 1:
        loss = -torch.log(predicted[i][1] * reward + gain[i])
      elif label[i] == 2:
        loss = -torch.log(predicted[i][2] * reward + gain[i])
      elif label[i] == 3:
        loss = -torch.log(predicted[i][3] * reward + gain[i])
      else:
        loss = -torch.log(predicted[i][4] * reward + gain[i])
      #if self.weight:
       # loss = self.weight[label[i]] * loss

      batch_loss += loss
      #print(loss)
      # 整个 batch 的总损失是否要求平均
    if self.size_average == True:
      batch_loss /= predicted.shape[0]

    
    #print(batch_loss)
    
    return batch_loss

        
    # def __call__(self, input, target): #self, true_label, predict_label, g
    #     """
    #     计算损失
    #     这个方法让类的实例表现的像函数一样，像函数一样可以调用

    #     :param input: (batch_size, C)，C是类别的总数
    #     :param target: (batch_size, 1)
    #     :return: 损失
    #     """

    #     batch_loss = 0.
    #     for i in range(input.shape[0]):

    #         numerator = np.exp(input[i, target[i]])     # 分子
    #         denominator = np.sum(np.exp(input[i, :]))   # 分母

    #         # 计算单个损失
    #         loss = -np.log(numerator / denominator)
    #         if self.weight:
    #             loss = self.weight[target[i]] * loss
    #         print("单个损失： ",loss)

    #         # 损失累加
    #         batch_loss += loss

    #     # 整个 batch 的总损失是否要求平均
    #     if self.size_average == True:
    #         batch_loss /= input.shape[0]

    #     return batch_loss

import matplotlib.pyplot as plt
plt.hist(y3553.numpy())

import matplotlib.pyplot as plt
plt.hist(y170.numpy())

import matplotlib.pyplot as plt
plt.hist(yr.numpy())

"""### Train

## Original Reddit
"""

import gc
import copy
import warnings
import random
import math
warnings.filterwarnings("ignore")

def setup_seed(seed):
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    np.random.seed(seed)
    random.seed(seed)
    torch.backends.cudnn.deterministic = True
    
# 设置随机数种子
setup_seed(1)

lr= 0.001
epochs = 20
batch_size = 17

reward = 0.6
tao = 0.6

# tao在train中明确不会用到 loss中计算所有data 而不是只计算阈值内的 tao只在test里用 选定合适的cov作为超参数 用tao凑 仅可取该cov下的f1作为结果
# reward是唯一可调节网络的超参数
# 目的是找到一个好用的g 能让gambler loss 里 加上g后所有的loss和都尽可能接近0 一旦某个data的预测值偏离过远 导致loss极大
# 神经网络为了降低loss 就会倾向于让整体的预测更准确（log中第一项） 或者让这些偏离过远的data得到更高的g值 也就是更高的log值 单个loss更小（）
# 从结果上说 可以用g来表示一个预测的不靠谱程度
# 在test中 把预测值的g从小到大排序 取前85% 就是cov占比
input_size = 768
hidden_size = 128
hidden_size2 = 32
n_layers = 2
num_classes = 4
pretrain_embedding = bert_model
bidirectional = True #这里为True，为双向LSTM

DATA_DIR = '/content/drive/MyDrive/CS40-2/CS40-2 Data Collections/'
model_name = 'SASI-original'

from torch.utils.data import TensorDataset, DataLoader
from sklearn.model_selection import KFold,StratifiedKFold

results = pd.DataFrame(columns=['fold','precision','recall','f1-score','class 0 f1','class 1 f1','class 2 f1','class 3 f1'])

## 定义train和test, cross_domain的时候交换

from sklearn.model_selection import train_test_split



X_train, X_test, y_train, y_test = train_test_split(xr, yr, test_size=0.3,random_state=4)

#X_train = x170
#y_train = y170
# X_train = x6290
# y_train = y6290
# X_test = x6290
# y_test = y6290


train_data = TensorDataset(X_train, y_train)
test_data = TensorDataset(X_test, y_test)
train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size,drop_last=True)
test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size,drop_last=True)

model = bilstmAttnMLP(input_size, num_classes, hidden_size, hidden_size2, n_layers, pretrain_embedding, bidirectional).to(device)
optimizer = torch.optim.Adam(model.parameters(), lr=lr)
#criterion = nn.CrossEntropyLoss()
criterion = GamblerLoss()

best_metric = 0.0


for epoch in range(epochs):  
# Set the flag to training
  model.train()
  train_loss = []
  train_correct = 0
  refrain_len = 0.0001
  refrain_incorrect = 0
  train_label_list = []
  pred_label = []
  
  #这里加index 在loss里去掉与predicted去掉refrain的 单独列表refrain
  for data, label in train_loader:
    data = data.to(device)
    label = label.type(torch.LongTensor).to(device)
    # forward + backward + loss + optimize
    optimizer.zero_grad()
    outputs,gain_list = model(data)
    gain = torch.gather(gain_list, dim=1, index=label.unsqueeze(dim=1)).squeeze()

    #print(gain)
    # if gain[i] >= tao, predicted[i] = Refrain or 2 else predicted还是原来那么走
    predicted = torch.argmax(outputs, -1)

    train_label_list.append(label.cpu().numpy().tolist())
    pred_label.append(predicted)
    # implment tao here

    # Custom Loss Function for the Gambler's Loss
    #if len(outputs[gain<tao]) == 0:
    #  continue
    loss = criterion(label,outputs,reward,gain)
    #loss = criterion(outputs, label)

    train_loss.append(loss.item())
    train_correct += torch.sum(predicted[gain<tao] == label[gain<tao])
    refrain_len += len(gain[gain>=tao])
    refrain_incorrect += torch.sum(predicted[gain>=tao] != label[gain>=tao])
    loss.backward()
    optimizer.step()

    del outputs
    torch.cuda.empty_cache()
    '''else:
      loss = criterion(label,predicted,reward,gain)
      #loss = criterion(outputs, label)

      train_loss.append(loss.item())
      train_correct += torch.sum(predicted == label)
      #refrain_len += len(gain[gain>=tao])
      #refrain_incorrect += torch.sum(predicted[gain>=tao] != label[gain>=tao])
      loss.backward()
      optimizer.step()

      del outputs
      torch.cuda.empty_cache()'''

  tem1 = []
  for i in train_label_list:
    for j in i:
      tem1.append(j)
  tem2 = []
  for i in pred_label:
    for j in i:
      tem2.append(j.cpu())
  metrics = 0
  metrics = classification_report(tem1, tem2, labels=[0, 1,2,3], output_dict=True)
  metrics = metrics['macro avg']['f1-score']
  total_loss  = np.mean(train_loss)
  Fail_Safe_Rejects = refrain_incorrect/refrain_len

  print('Epoch: %d, loss: %.5f, macro-f1: %.5f, Fail-Safe_Rejects: %.4f' %(epoch + 1, total_loss,metrics,Fail_Safe_Rejects))
  print('\nTotal num of cov:',str(len(tem2)))
  print('Total num of refrain:',str(len(X_train)-len(tem2)))
  print('\n------------------------------------------------------------------------')
  if metrics > best_metric:
    best_metric = metrics
    best_model_wts = copy.deepcopy(model.state_dict())
  #print(best_metric)
print('Finished Training')

#test

gain_total = []
model.load_state_dict(best_model_wts)
with torch.no_grad():
  test_label_list = []
  pred_label = []
  refrain_len = 0.0001
  refrain_incorrect = 0
  for input_torch,test_label in test_loader:
    
    model.eval()
    input_torch = input_torch.to(device)
    outputs,gain_list = model(input_torch)
    
    for i in gain:
      gain_total.append(i)

    predicted = torch.argmax(outputs, -1)
    gain = torch.gather(gain_list, dim=1, index=label.unsqueeze(dim=1)).squeeze()
    label = test_label.to(device)
    test_label_list.append(label.cpu().numpy().tolist())
    pred_label.append(predicted)
    refrain_len += len(gain[gain>=tao])
    refrain_incorrect += torch.sum(predicted[gain>=tao] != label[gain>=tao])
    del outputs
    torch.cuda.empty_cache()
    '''
print("Starting testing")
tem1 = []
for i in test_label_list:
  for j in i:
    tem1.append(j)
tem2 = []
for i in pred_label:
  for j in i:
    tem2.append(j.cpu())
Fail_Safe_Rejects = refrain_incorrect/refrain_len
metrics = classification_report(tem1, tem2, labels=[0, 1,2,3], output_dict=True)
#print(metrics)
#results.loc[0] = [0,metrics['macro avg']['precision'],metrics['macro avg']['recall'],metrics['macro avg']['f1-score'],metrics['0']['f1-score'],metrics['1']['f1-score'],metrics['2']['f1-score'],metrics['3']['f1-score']]
#print('Fail-Safe_Rejects: %.6f' %(Fail_Safe_Rejects))
print('cov: %.3f' %(1-refrain_len/len(test_data)))
#results
# results.to_csv(DATA_DIR+f'{model_name}.csv')'''

"""# 170"""

import gc
import copy
import warnings
import random
import math
warnings.filterwarnings("ignore")

def setup_seed(seed):
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    np.random.seed(seed)
    random.seed(seed)
    torch.backends.cudnn.deterministic = True
    
# 设置随机数种子
setup_seed(1)

lr= 0.001
epochs = 20
batch_size = 17

reward = 0.4
tao = 0.4

# tao在train中明确不会用到 loss中计算所有data 而不是只计算阈值内的 tao只在test里用 选定合适的cov作为超参数 用tao凑 仅可取该cov下的f1作为结果
# reward是唯一可调节网络的超参数
# 目的是找到一个好用的g 能让gambler loss 里 加上g后所有的loss和都尽可能接近0 一旦某个data的预测值偏离过远 导致loss极大
# 神经网络为了降低loss 就会倾向于让整体的预测更准确（log中第一项） 或者让这些偏离过远的data得到更高的g值 也就是更高的log值 单个loss更小（）
# 从结果上说 可以用g来表示一个预测的不靠谱程度
# 在test中 把预测值的g从小到大排序 取前85% 就是cov占比
input_size = 768
hidden_size = 128
hidden_size2 = 32
n_layers = 2
num_classes = 4
pretrain_embedding = bert_model
bidirectional = True #这里为True，为双向LSTM

DATA_DIR = '/content/drive/MyDrive/CS40-2/CS40-2 Data Collections/'
model_name = 'SASI-170'

from torch.utils.data import TensorDataset, DataLoader
from sklearn.model_selection import KFold,StratifiedKFold

results = pd.DataFrame(columns=['fold','precision','recall','f1-score','class 0 f1','class 1 f1','class 2 f1','class 3 f1'])

## 定义train和test, cross_domain的时候交换

from sklearn.model_selection import train_test_split



X_train, X_test, y_train, y_test = train_test_split(x170, y170, test_size=0.5,random_state=4)

#X_train = x170
#y_train = y170
# X_train = x6290
# y_train = y6290
# X_test = x6290
# y_test = y6290


train_data = TensorDataset(X_train, y_train)
test_data = TensorDataset(X_test, y_test)
train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size,drop_last=True)
test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size,drop_last=True)

model = bilstmAttnMLP(input_size, num_classes, hidden_size, hidden_size2, n_layers, pretrain_embedding, bidirectional).to(device)
optimizer = torch.optim.Adam(model.parameters(), lr=lr)
#criterion = nn.CrossEntropyLoss()
criterion = GamblerLoss()

best_metric = 0.0


for epoch in range(epochs):  
# Set the flag to training
  model.train()
  train_loss = []
  train_correct = 0
  refrain_len = 0.0001
  refrain_incorrect = 0
  train_label_list = []
  pred_label = []
  
  #这里加index 在loss里去掉与predicted去掉refrain的 单独列表refrain
  for data, label in train_loader:
    data = data.to(device)
    label = label.type(torch.LongTensor).to(device)
    # forward + backward + loss + optimize
    optimizer.zero_grad()
    outputs,gain_list = model(data)
    gain = torch.gather(gain_list, dim=1, index=label.unsqueeze(dim=1)).squeeze()

    #print(gain)
    # if gain[i] >= tao, predicted[i] = Refrain or 2 else predicted还是原来那么走
    predicted = torch.argmax(outputs, -1)

    train_label_list.append(label.cpu().numpy().tolist())
    pred_label.append(predicted)
    # implment tao here

    # Custom Loss Function for the Gambler's Loss
    #if len(outputs[gain<tao]) == 0:
    #  continue
    loss = criterion(label,outputs,reward,gain)
    #loss = criterion(outputs, label)

    train_loss.append(loss.item())
    train_correct += torch.sum(predicted[gain<tao] == label[gain<tao])
    refrain_len += len(gain[gain>=tao])
    refrain_incorrect += torch.sum(predicted[gain>=tao] != label[gain>=tao])
    loss.backward()
    optimizer.step()

    del outputs
    torch.cuda.empty_cache()
    '''else:
      loss = criterion(label,predicted,reward,gain)
      #loss = criterion(outputs, label)

      train_loss.append(loss.item())
      train_correct += torch.sum(predicted == label)
      #refrain_len += len(gain[gain>=tao])
      #refrain_incorrect += torch.sum(predicted[gain>=tao] != label[gain>=tao])
      loss.backward()
      optimizer.step()

      del outputs
      torch.cuda.empty_cache()'''

  tem1 = []
  for i in train_label_list:
    for j in i:
      tem1.append(j)
  tem2 = []
  for i in pred_label:
    for j in i:
      tem2.append(j.cpu())
  metrics = 0
  metrics = classification_report(tem1, tem2, labels=[0, 1,2,3], output_dict=True)
  metrics = metrics['macro avg']['f1-score']
  total_loss  = np.mean(train_loss)
  Fail_Safe_Rejects = refrain_incorrect/refrain_len

  print('Epoch: %d, loss: %.5f, macro-f1: %.5f, Fail-Safe_Rejects: %.4f' %(epoch + 1, total_loss,metrics,Fail_Safe_Rejects))
  print('\nTotal num of cov:',str(len(tem2)))
  print('Total num of refrain:',str(len(X_train)-len(tem2)))
  print('\n------------------------------------------------------------------------')
  if metrics > best_metric:
    best_metric = metrics
    best_model_wts = copy.deepcopy(model.state_dict())
  #print(best_metric)
print('Finished Training')

#test

gain_total = []
model.load_state_dict(best_model_wts)
with torch.no_grad():
  test_label_list = []
  pred_label = []
  refrain_len = 0.0001
  refrain_incorrect = 0
  for input_torch,test_label in test_loader:
    
    model.eval()
    input_torch = input_torch.to(device)
    outputs,gain_list = model(input_torch)
    
    for i in gain:
      gain_total.append(i)

    predicted = torch.argmax(outputs, -1)
    gain = torch.gather(gain_list, dim=1, index=label.unsqueeze(dim=1)).squeeze()
    label = test_label.to(device)
    test_label_list.append(label.cpu().numpy().tolist())
    pred_label.append(predicted)
    refrain_len += len(gain[gain>=tao])
    refrain_incorrect += torch.sum(predicted[gain>=tao] != label[gain>=tao])
    del outputs
    torch.cuda.empty_cache()
    '''
print("Starting testing")
tem1 = []
for i in test_label_list:
  for j in i:
    tem1.append(j)
tem2 = []
for i in pred_label:
  for j in i:
    tem2.append(j.cpu())
Fail_Safe_Rejects = refrain_incorrect/refrain_len
metrics = classification_report(tem1, tem2, labels=[0, 1,2,3], output_dict=True)
#print(metrics)
#results.loc[0] = [0,metrics['macro avg']['precision'],metrics['macro avg']['recall'],metrics['macro avg']['f1-score'],metrics['0']['f1-score'],metrics['1']['f1-score'],metrics['2']['f1-score'],metrics['3']['f1-score']]
#print('Fail-Safe_Rejects: %.6f' %(Fail_Safe_Rejects))
print('cov: %.3f' %(1-refrain_len/len(test_data)))
#results
# results.to_csv(DATA_DIR+f'{model_name}.csv')'''

print(len(gain_total))
#这个是tao的数量
#下面用i%2控制tao的数量

x = []
index=sorted(range(len(gain_total)), key=lambda k: gain_total[k])
for i in index:
  # 如果是test 3553就把2改成50以上
  if i % 2 == 0:
    x.append(gain_total[i])
tao_list = x
setup_seed(1)
fsr = []
cov = []
f1 = []
for tao in tao_list:
  model.load_state_dict(best_model_wts)
  with torch.no_grad():
    test_label_list = []
    pred_label = []
    refrain_len = 0.0001
    refrain_incorrect = 0
    for input_torch,test_label in test_loader:
      
      model.eval()
      input_torch = input_torch.to(device)
      outputs,gain_list = model(input_torch)
      predicted = torch.argmax(outputs, -1)
      gain = torch.gather(gain_list, dim=1, index=predicted.unsqueeze(dim=1)).squeeze()

      #predicted = torch.argmax(outputs, -1)
      label = test_label.type(torch.LongTensor).to(device)
      test_label_list.append(label[gain<tao].cpu().numpy().tolist())
      pred_label.append(predicted[gain<tao])
      refrain_len += len(gain[gain>=tao])
      refrain_incorrect += torch.sum(predicted[gain>=tao] != label[gain>=tao])
      del outputs
      torch.cuda.empty_cache()
  
  tem1 = []
  for i in test_label_list:
    for j in i:
      tem1.append(j)
  tem2 = []
  for i in pred_label:
    for j in i:
      tem2.append(j.cpu())
  Fail_Safe_Rejects = refrain_incorrect/refrain_len
  metrics = classification_report(tem1, tem2, labels=[0, 1,2,3], output_dict=True)
  fsr.append(Fail_Safe_Rejects.cpu())
  cov.append((1-refrain_len/len(test_data)))
  f1.append(metrics['macro avg']['f1-score'])
  #results.loc[0] = [0,metrics['macro avg']['precision'],metrics['macro avg']['recall'],metrics['macro avg']['f1-score'],metrics['0']['f1-score'],metrics['1']['f1-score']]
  #print('Fail-Safe_Rejects: %.6f' %(Fail_Safe_Rejects),'cov: %.3f' %(1-refrain_len/len(test_data)),'macro-f1: %.3f' %metrics['macro avg']['f1-score'])
  #print('cov: %.3f' %(1-refrain_len/len(test_data)))
  #print(results)
  # results.to_csv(DATA_DIR+f'{model_name}.csv')

index = 0
min = 1
index_cov_85 = 0
for i in cov:
  if (i - 0.85)**2 < min:
    min = (i - 0.85)**2
    index_cov_85 = index
  index += 1
print('85% cov f1 : ',f1[index_cov_85])

import numpy as np
import matplotlib.pyplot as plt
fig = plt.figure(figsize=(15, 10), dpi=80)
index=sorted(range(len(cov)), key=lambda k: cov[k])
x=[]
y1=[]
y2=[]
for i in index:
  
  x.append(cov[i]*100)
  y1.append(fsr[i]*100)
  y2.append(f1[i]*100)

  # 限定纵轴的范围
plt.plot(x, y1, marker='o',label=u'fail_safe_rejects')
plt.plot(x, y2, marker='o',label=u'macro_f1')
plt.legend()
plt.xlabel("cov(%)")
plt.ylabel('performance(%)')
plt.title("depr-170")
plt.ylim(0,100)
plt.axvline(x=cov[index_cov_85]*100, ls='--', c='red')
plt.axhline(y=f1[index_cov_85]*100, ls='--', c='black')
plt.show()
#x为候选cov y为f1
#假设超参数cov为0.85
#取x中cov与0.85差值最小者 对应的y为最终结果

import numpy as np
import matplotlib.pyplot as plt
fig = plt.figure(figsize=(12, 8), dpi=80)
x = [i.cpu() for i in tao_list]
y = cov
plt.plot(x, y, marker='o',label=u'cov')

plt.legend()
plt.xlabel("tao")
plt.title("depr-170")
plt.show()

#更改index值来找
index = range(0,5)
model.to(device)
outputs,gain_list = model(x170[index].to(device))
print('85% cov tao: ',tao_list[index_cov_85].cpu().numpy())
post = np.array(text170)[index]
pred = torch.argmax(outputs, -1)
label = y170[index].type(torch.LongTensor).to(device)
gain = torch.gather(gain_list, dim=1, index=pred.unsqueeze(dim=1)).squeeze()
for i in range(len(index)):
  print('-------------------------------')
  print('post:')
  print(post[i])
  print('pred:')
  print(pred[i].cpu().numpy())
  print('true_label:')
  print(label[i].cpu().numpy())
  print('g value:')
  print(gain[i].detach().cpu().numpy())
  if gain[i]>=tao_list[index_cov_85]:
    print('Retrain: TRUE')
  else:
    print('Retrain: FALSE')

"""# 3553"""

import gc
import copy
import warnings
import random
import math
warnings.filterwarnings("ignore")

def setup_seed(seed):
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    np.random.seed(seed)
    random.seed(seed)
    torch.backends.cudnn.deterministic = True
    
# 设置随机数种子
setup_seed(1)

lr= 0.001
epochs = 10
batch_size = 256

reward = 0.4
tao = 0.4

# tao在train中明确不会用到 loss中计算所有data 而不是只计算阈值内的 tao只在test里用 选定合适的cov作为超参数 用tao凑 仅可取该cov下的f1作为结果
# reward是唯一可调节网络的超参数
# 目的是找到一个好用的g 能让gambler loss 里 加上g后所有的loss和都尽可能接近0 一旦某个data的预测值偏离过远 导致loss极大
# 神经网络为了降低loss 就会倾向于让整体的预测更准确（log中第一项） 或者让这些偏离过远的data得到更高的g值 也就是更高的log值 单个loss更小（）
# 从结果上说 可以用g来表示一个预测的不靠谱程度
# 在test中 把预测值的g从小到大排序 取前85% 就是cov占比
input_size = 768
hidden_size = 128
hidden_size2 = 32
n_layers = 2
num_classes = 4
pretrain_embedding = bert_model
bidirectional = True #这里为True，为双向LSTM

DATA_DIR = '/content/drive/MyDrive/CS40-2/CS40-2 Data Collections/'
model_name = 'SASI-170'

from torch.utils.data import TensorDataset, DataLoader
from sklearn.model_selection import KFold,StratifiedKFold

results = pd.DataFrame(columns=['fold','precision','recall','f1-score','class 0 f1','class 1 f1','class 2 f1','class 3 f1'])

## 定义train和test, cross_domain的时候交换

from sklearn.model_selection import train_test_split



X_train, X_test, y_train, y_test = train_test_split(x3553, y3553, test_size=0.2,random_state=4)

#X_train = x170
#y_train = y170
# X_train = x6290
# y_train = y6290
# X_test = x6290
# y_test = y6290


train_data = TensorDataset(X_train, y_train)
test_data = TensorDataset(X_test, y_test)
train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size,drop_last=True)
test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size,drop_last=True)

model = bilstmAttnMLP(input_size, num_classes, hidden_size, hidden_size2, n_layers, pretrain_embedding, bidirectional).to(device)
optimizer = torch.optim.Adam(model.parameters(), lr=lr)
#criterion = nn.CrossEntropyLoss()
criterion = GamblerLoss()

best_metric = 0.0


for epoch in range(epochs):  
# Set the flag to training
  model.train()
  train_loss = []
  train_correct = 0
  refrain_len = 0.0001
  refrain_incorrect = 0
  train_label_list = []
  pred_label = []
  
  #这里加index 在loss里去掉与predicted去掉refrain的 单独列表refrain
  for data, label in train_loader:
    data = data.to(device)
    label = label.type(torch.LongTensor).to(device)
    # forward + backward + loss + optimize
    optimizer.zero_grad()
    outputs,gain_list = model(data)
    gain = torch.gather(gain_list, dim=1, index=label.unsqueeze(dim=1)).squeeze()

    #print(gain)
    # if gain[i] >= tao, predicted[i] = Refrain or 2 else predicted还是原来那么走
    predicted = torch.argmax(outputs, -1)

    train_label_list.append(label.cpu().numpy().tolist())
    pred_label.append(predicted)
    # implment tao here

    # Custom Loss Function for the Gambler's Loss
    #if len(outputs[gain<tao]) == 0:
    #  continue
    loss = criterion(label,outputs,reward,gain)
    #loss = criterion(outputs, label)

    train_loss.append(loss.item())
    train_correct += torch.sum(predicted[gain<tao] == label[gain<tao])
    refrain_len += len(gain[gain>=tao])
    refrain_incorrect += torch.sum(predicted[gain>=tao] != label[gain>=tao])
    loss.backward()
    optimizer.step()

    del outputs
    torch.cuda.empty_cache()
    '''else:
      loss = criterion(label,predicted,reward,gain)
      #loss = criterion(outputs, label)

      train_loss.append(loss.item())
      train_correct += torch.sum(predicted == label)
      #refrain_len += len(gain[gain>=tao])
      #refrain_incorrect += torch.sum(predicted[gain>=tao] != label[gain>=tao])
      loss.backward()
      optimizer.step()

      del outputs
      torch.cuda.empty_cache()'''

  tem1 = []
  for i in train_label_list:
    for j in i:
      tem1.append(j)
  tem2 = []
  for i in pred_label:
    for j in i:
      tem2.append(j.cpu())
  metrics = 0
  metrics = classification_report(tem1, tem2, labels=[0, 1,2,3], output_dict=True)
  metrics = metrics['macro avg']['f1-score']
  total_loss  = np.mean(train_loss)
  Fail_Safe_Rejects = refrain_incorrect/refrain_len

  print('Epoch: %d, loss: %.5f, macro-f1: %.5f, Fail-Safe_Rejects: %.4f' %(epoch + 1, total_loss,metrics,Fail_Safe_Rejects))
  print('\nTotal num of cov:',str(len(tem2)))
  print('Total num of refrain:',str(len(X_train)-len(tem2)))
  print('\n------------------------------------------------------------------------')
  if metrics > best_metric:
    best_metric = metrics
    best_model_wts = copy.deepcopy(model.state_dict())
  #print(best_metric)
print('Finished Training')

#test

gain_total = []
model.load_state_dict(best_model_wts)
with torch.no_grad():
  test_label_list = []
  pred_label = []
  refrain_len = 0.0001
  refrain_incorrect = 0
  for input_torch,test_label in test_loader:
    
    model.eval()
    input_torch = input_torch.to(device)
    outputs,gain_list = model(input_torch)
    
    for i in gain:
      gain_total.append(i)

    predicted = torch.argmax(outputs, -1)
    gain = torch.gather(gain_list, dim=1, index=label.unsqueeze(dim=1)).squeeze()
    label = test_label.to(device)
    test_label_list.append(label.cpu().numpy().tolist())
    pred_label.append(predicted)
    refrain_len += len(gain[gain>=tao])
    refrain_incorrect += torch.sum(predicted[gain>=tao] != label[gain>=tao])
    del outputs
    torch.cuda.empty_cache()
    '''
print("Starting testing")
tem1 = []
for i in test_label_list:
  for j in i:
    tem1.append(j)
tem2 = []
for i in pred_label:
  for j in i:
    tem2.append(j.cpu())
Fail_Safe_Rejects = refrain_incorrect/refrain_len
metrics = classification_report(tem1, tem2, labels=[0, 1,2,3], output_dict=True)
#print(metrics)
#results.loc[0] = [0,metrics['macro avg']['precision'],metrics['macro avg']['recall'],metrics['macro avg']['f1-score'],metrics['0']['f1-score'],metrics['1']['f1-score'],metrics['2']['f1-score'],metrics['3']['f1-score']]
#print('Fail-Safe_Rejects: %.6f' %(Fail_Safe_Rejects))
print('cov: %.3f' %(1-refrain_len/len(test_data)))
#results
# results.to_csv(DATA_DIR+f'{model_name}.csv')'''

x = []
index=sorted(range(len(gain_total)), key=lambda k: gain_total[k])
for i in index:
  # 如果是test 3553就把2改成50以上
  if i % 50 == 0:
    x.append(gain_total[i])
tao_list = x
setup_seed(1)
fsr = []
cov = []
f1 = []
for tao in tao_list:
  model.load_state_dict(best_model_wts)
  with torch.no_grad():
    test_label_list = []
    pred_label = []
    refrain_len = 0.0001
    refrain_incorrect = 0
    for input_torch,test_label in test_loader:
      
      model.eval()
      input_torch = input_torch.to(device)
      outputs,gain_list = model(input_torch)
      predicted = torch.argmax(outputs, -1)
      gain = torch.gather(gain_list, dim=1, index=predicted.unsqueeze(dim=1)).squeeze()

      #predicted = torch.argmax(outputs, -1)
      label = test_label.type(torch.LongTensor).to(device)
      test_label_list.append(label[gain<tao].cpu().numpy().tolist())
      pred_label.append(predicted[gain<tao])
      refrain_len += len(gain[gain>=tao])
      refrain_incorrect += torch.sum(predicted[gain>=tao] != label[gain>=tao])
      del outputs
      torch.cuda.empty_cache()
  
  tem1 = []
  for i in test_label_list:
    for j in i:
      tem1.append(j)
  tem2 = []
  for i in pred_label:
    for j in i:
      tem2.append(j.cpu())
  Fail_Safe_Rejects = refrain_incorrect/refrain_len
  metrics = classification_report(tem1, tem2, labels=[0, 1,2,3], output_dict=True)
  fsr.append(Fail_Safe_Rejects.cpu())
  cov.append((1-refrain_len/len(test_data)))
  f1.append(metrics['macro avg']['f1-score'])
  #results.loc[0] = [0,metrics['macro avg']['precision'],metrics['macro avg']['recall'],metrics['macro avg']['f1-score'],metrics['0']['f1-score'],metrics['1']['f1-score']]
  #print('Fail-Safe_Rejects: %.6f' %(Fail_Safe_Rejects),'cov: %.3f' %(1-refrain_len/len(test_data)),'macro-f1: %.3f' %metrics['macro avg']['f1-score'])
  #print('cov: %.3f' %(1-refrain_len/len(test_data)))
  #print(results)
  # results.to_csv(DATA_DIR+f'{model_name}.csv')

index = 0
min = 1
index_cov_85 = 0
for i in cov:
  if (i - 0.85)**2 < min:
    min = (i - 0.85)**2
    index_cov_85 = index
  index += 1
print('85% cov f1 : ',f1[index_cov_85])

import numpy as np
import matplotlib.pyplot as plt
fig = plt.figure(figsize=(15, 10), dpi=80)
index=sorted(range(len(cov)), key=lambda k: cov[k])
x=[]
y1=[]
y2=[]
for i in index:
  
  x.append(cov[i]*100)
  y1.append(fsr[i]*100)
  y2.append(f1[i]*100)

  # 限定纵轴的范围
plt.plot(x, y1, marker='o',label=u'fail_safe_rejects')
plt.plot(x, y2, marker='o',label=u'macro_f1')
plt.legend()
plt.xlabel("cov(%)")
plt.ylabel('performance(%)')
plt.title("depr-170")
plt.ylim(0,100)
plt.axvline(x=cov[index_cov_85]*100, ls='--', c='red')
plt.axhline(y=f1[index_cov_85]*100, ls='--', c='black')
plt.show()
#x为候选cov y为f1
#假设超参数cov为0.85
#取x中cov与0.85差值最小者 对应的y为最终结果

import numpy as np
import matplotlib.pyplot as plt
fig = plt.figure(figsize=(12, 8), dpi=80)
x = [i.cpu().detach() for i in tao_list]
y = cov
plt.plot(x, y, marker='o',label=u'cov')

plt.legend()
plt.xlabel("tao")
plt.title("depr-170")
plt.show()

#更改index值来找
index = range(0,5)
model.to(device)
outputs,gain_list = model(x3553[index].to(device))
print('85% cov tao: ',tao_list[index_cov_85].cpu().numpy())
post = np.array(text3553)[index]
pred = torch.argmax(outputs, -1)
label = y3553[index].type(torch.LongTensor).to(device)
gain = torch.gather(gain_list, dim=1, index=pred.unsqueeze(dim=1)).squeeze()
for i in range(len(index)):
  print('-------------------------------')
  print('post:')
  print(post[i])
  print('pred:')
  print(pred[i].cpu().numpy())
  print('true_label:')
  print(label[i].cpu().numpy())
  print('g value:')
  print(gain[i].detach().cpu().numpy())
  if gain[i]>=tao_list[index_cov_85]:
    print('Retrain: TRUE')
  else:
    print('Retrain: FALSE')

"""# 170train 3553test"""

import gc
import copy
import warnings
import random
import math
warnings.filterwarnings("ignore")

def setup_seed(seed):
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    np.random.seed(seed)
    random.seed(seed)
    torch.backends.cudnn.deterministic = True
    
# 设置随机数种子
setup_seed(1)

lr= 0.001
epochs = 20
batch_size = 17

reward = 0.4
tao = 0.4

# tao在train中明确不会用到 loss中计算所有data 而不是只计算阈值内的 tao只在test里用 选定合适的cov作为超参数 用tao凑 仅可取该cov下的f1作为结果
# reward是唯一可调节网络的超参数
# 目的是找到一个好用的g 能让gambler loss 里 加上g后所有的loss和都尽可能接近0 一旦某个data的预测值偏离过远 导致loss极大
# 神经网络为了降低loss 就会倾向于让整体的预测更准确（log中第一项） 或者让这些偏离过远的data得到更高的g值 也就是更高的log值 单个loss更小（）
# 从结果上说 可以用g来表示一个预测的不靠谱程度
# 在test中 把预测值的g从小到大排序 取前85% 就是cov占比
input_size = 768
hidden_size = 128
hidden_size2 = 32
n_layers = 2
num_classes = 4
pretrain_embedding = bert_model
bidirectional = True #这里为True，为双向LSTM

DATA_DIR = '/content/drive/MyDrive/CS40-2/CS40-2 Data Collections/'
model_name = 'SASI-170'

from torch.utils.data import TensorDataset, DataLoader
from sklearn.model_selection import KFold,StratifiedKFold

results = pd.DataFrame(columns=['fold','precision','recall','f1-score','class 0 f1','class 1 f1','class 2 f1','class 3 f1'])

## 定义train和test, cross_domain的时候交换

from sklearn.model_selection import train_test_split



#X_train, X_test, y_train, y_test = train_test_split(x170, y170, test_size=0.5,random_state=4)

X_train = x170
y_train = y170
X_test = x3553
y_test = y3553
# X_train = x6290
# y_train = y6290
# X_test = x6290
# y_test = y6290


train_data = TensorDataset(X_train, y_train)
test_data = TensorDataset(X_test, y_test)
train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size,drop_last=True)
test_loader = DataLoader(test_data, shuffle=True, batch_size=256,drop_last=True)

model = bilstmAttnMLP(input_size, num_classes, hidden_size, hidden_size2, n_layers, pretrain_embedding, bidirectional).to(device)
optimizer = torch.optim.Adam(model.parameters(), lr=lr)
#criterion = nn.CrossEntropyLoss()
criterion = GamblerLoss()

best_metric = 0.0


for epoch in range(epochs):  
# Set the flag to training
  model.train()
  train_loss = []
  train_correct = 0
  refrain_len = 0.0001
  refrain_incorrect = 0
  train_label_list = []
  pred_label = []
  
  #这里加index 在loss里去掉与predicted去掉refrain的 单独列表refrain
  for data, label in train_loader:
    data = data.to(device)
    label = label.type(torch.LongTensor).to(device)
    # forward + backward + loss + optimize
    optimizer.zero_grad()
    outputs,gain_list = model(data)
    gain = torch.gather(gain_list, dim=1, index=label.unsqueeze(dim=1)).squeeze()

    #print(gain)
    # if gain[i] >= tao, predicted[i] = Refrain or 2 else predicted还是原来那么走
    predicted = torch.argmax(outputs, -1)

    train_label_list.append(label.cpu().numpy().tolist())
    pred_label.append(predicted)
    # implment tao here

    # Custom Loss Function for the Gambler's Loss
    #if len(outputs[gain<tao]) == 0:
    #  continue
    loss = criterion(label,outputs,reward,gain)
    #loss = criterion(outputs, label)

    train_loss.append(loss.item())
    train_correct += torch.sum(predicted[gain<tao] == label[gain<tao])
    refrain_len += len(gain[gain>=tao])
    refrain_incorrect += torch.sum(predicted[gain>=tao] != label[gain>=tao])
    loss.backward()
    optimizer.step()

    del outputs
    torch.cuda.empty_cache()
    '''else:
      loss = criterion(label,predicted,reward,gain)
      #loss = criterion(outputs, label)

      train_loss.append(loss.item())
      train_correct += torch.sum(predicted == label)
      #refrain_len += len(gain[gain>=tao])
      #refrain_incorrect += torch.sum(predicted[gain>=tao] != label[gain>=tao])
      loss.backward()
      optimizer.step()

      del outputs
      torch.cuda.empty_cache()'''

  tem1 = []
  for i in train_label_list:
    for j in i:
      tem1.append(j)
  tem2 = []
  for i in pred_label:
    for j in i:
      tem2.append(j.cpu())
  metrics = 0
  metrics = classification_report(tem1, tem2, labels=[0, 1,2,3], output_dict=True)
  metrics = metrics['macro avg']['f1-score']
  total_loss  = np.mean(train_loss)
  Fail_Safe_Rejects = refrain_incorrect/refrain_len

  print('Epoch: %d, loss: %.5f, macro-f1: %.5f, Fail-Safe_Rejects: %.4f' %(epoch + 1, total_loss,metrics,Fail_Safe_Rejects))
  print('\nTotal num of cov:',str(len(tem2)))
  print('Total num of refrain:',str(len(X_train)-len(tem2)))
  print('\n------------------------------------------------------------------------')
  if metrics > best_metric:
    best_metric = metrics
    best_model_wts = copy.deepcopy(model.state_dict())
  #print(best_metric)
print('Finished Training')

#test

gain_total = []
model.load_state_dict(best_model_wts)
with torch.no_grad():
  test_label_list = []
  pred_label = []
  refrain_len = 0.0001
  refrain_incorrect = 0
  for input_torch,test_label in test_loader:
    
    model.eval()
    input_torch = input_torch.to(device)
    outputs,gain_list = model(input_torch)
    
    for i in gain:
      gain_total.append(i)

    predicted = torch.argmax(outputs, -1)
    gain = torch.gather(gain_list, dim=1, index=label.unsqueeze(dim=1)).squeeze()
    label = test_label.to(device)
    test_label_list.append(label.cpu().numpy().tolist())
    pred_label.append(predicted)
    refrain_len += len(gain[gain>=tao])
    refrain_incorrect += torch.sum(predicted[gain>=tao] != label[gain>=tao])
    del outputs
    torch.cuda.empty_cache()
    
print("Starting testing")
tem1 = []
for i in test_label_list:
  for j in i:
    tem1.append(j)
tem2 = []
for i in pred_label:
  for j in i:
    tem2.append(j.cpu())
Fail_Safe_Rejects = refrain_incorrect/refrain_len
metrics = classification_report(tem1, tem2, labels=[0, 1,2,3], output_dict=True)
#print(metrics)
#results.loc[0] = [0,metrics['macro avg']['precision'],metrics['macro avg']['recall'],metrics['macro avg']['f1-score'],metrics['0']['f1-score'],metrics['1']['f1-score'],metrics['2']['f1-score'],metrics['3']['f1-score']]
#print('Fail-Safe_Rejects: %.6f' %(Fail_Safe_Rejects))
print('cov: %.3f' %(1-refrain_len/len(test_data)))
#results
# results.to_csv(DATA_DIR+f'{model_name}.csv')'''

gain_total = []
model.load_state_dict(best_model_wts)
with torch.no_grad():
  test_label_list = []
  pred_label = []
  refrain_len = 0.0001
  refrain_incorrect = 0
  for input_torch,test_label in test_loader:
    
    model.eval()
    input_torch = input_torch.to(device)
    outputs,gain_list = model(input_torch)
    
    for i in gain:
      gain_total.append(i)

    predicted = torch.argmax(outputs, -1)
    gain = torch.gather(gain_list, dim=1, index=label.unsqueeze(dim=1)).squeeze()
    label = test_label.to(device)
    test_label_list.append(label.cpu().numpy().tolist())
    pred_label.append(predicted)
    refrain_len += len(gain[gain>=tao])
    refrain_incorrect += torch.sum(predicted[gain>=tao] != label[gain>=tao])
    del outputs
    torch.cuda.empty_cache()
    
print("Starting testing")
tem1 = []
for i in test_label_list:
  for j in i:
    tem1.append(j)
tem2 = []
for i in pred_label:
  for j in i:
    tem2.append(j.cpu())
Fail_Safe_Rejects = refrain_incorrect/refrain_len
metrics = classification_report(tem1, tem2, labels=[0, 1,2,3], output_dict=True)
#print(metrics)

metrics

x = []
index=sorted(range(len(gain_total)), key=lambda k: gain_total[k])
for i in index:
  # 如果是test 3553就把2改成50以上
  if i % 150 == 0:
    x.append(gain_total[i])
tao_list = x
setup_seed(1)
fsr = []
cov = []
f1 = []
aa=0
for tao in tao_list:
  aa+=1
  if aa%5==0:
    print(aa)
  model.load_state_dict(best_model_wts)
  with torch.no_grad():
    test_label_list = []
    pred_label = []
    refrain_len = 0.0001
    refrain_incorrect = 0
    for input_torch,test_label in test_loader:
      
      model.eval()
      input_torch = input_torch.to(device)
      outputs,gain_list = model(input_torch)
      predicted = torch.argmax(outputs, -1)
      gain = torch.gather(gain_list, dim=1, index=predicted.unsqueeze(dim=1)).squeeze()

      #predicted = torch.argmax(outputs, -1)
      label = test_label.type(torch.LongTensor).to(device)
      test_label_list.append(label[gain<tao].cpu().numpy().tolist())
      pred_label.append(predicted[gain<tao])
      refrain_len += len(gain[gain>=tao])
      refrain_incorrect += torch.sum(predicted[gain>=tao] != label[gain>=tao])
      del outputs
      torch.cuda.empty_cache()
  
  tem1 = []
  for i in test_label_list:
    for j in i:
      tem1.append(j)
  tem2 = []
  for i in pred_label:
    for j in i:
      tem2.append(j.cpu())
  Fail_Safe_Rejects = refrain_incorrect/refrain_len
  metrics = classification_report(tem1, tem2, labels=[0, 1,2,3], output_dict=True)
  fsr.append(Fail_Safe_Rejects.cpu())
  cov.append((1-refrain_len/len(test_data)))
  f1.append(metrics['macro avg']['f1-score'])
  #results.loc[0] = [0,metrics['macro avg']['precision'],metrics['macro avg']['recall'],metrics['macro avg']['f1-score'],metrics['0']['f1-score'],metrics['1']['f1-score']]
  #print('Fail-Safe_Rejects: %.6f' %(Fail_Safe_Rejects),'cov: %.3f' %(1-refrain_len/len(test_data)),'macro-f1: %.3f' %metrics['macro avg']['f1-score'])
  #print('cov: %.3f' %(1-refrain_len/len(test_data)))
  #print(results)
  # results.to_csv(DATA_DIR+f'{model_name}.csv')

index = 0
min = 1
index_cov_85 = 0
for i in cov:
  if (i - 0.85)**2 < min:
    min = (i - 0.85)**2
    index_cov_85 = index
  index += 1
print('85% cov f1 : ',f1[index_cov_85])

"""# 3553train 170test"""

import gc
import copy
import warnings
import random
import math
warnings.filterwarnings("ignore")

def setup_seed(seed):
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    np.random.seed(seed)
    random.seed(seed)
    torch.backends.cudnn.deterministic = True
    
# 设置随机数种子
setup_seed(1)

lr= 0.001
epochs = 20
batch_size = 256

reward = 0.4
tao = 0.4

# tao在train中明确不会用到 loss中计算所有data 而不是只计算阈值内的 tao只在test里用 选定合适的cov作为超参数 用tao凑 仅可取该cov下的f1作为结果
# reward是唯一可调节网络的超参数
# 目的是找到一个好用的g 能让gambler loss 里 加上g后所有的loss和都尽可能接近0 一旦某个data的预测值偏离过远 导致loss极大
# 神经网络为了降低loss 就会倾向于让整体的预测更准确（log中第一项） 或者让这些偏离过远的data得到更高的g值 也就是更高的log值 单个loss更小（）
# 从结果上说 可以用g来表示一个预测的不靠谱程度
# 在test中 把预测值的g从小到大排序 取前85% 就是cov占比
input_size = 768
hidden_size = 128
hidden_size2 = 32
n_layers = 2
num_classes = 4
pretrain_embedding = bert_model
bidirectional = True #这里为True，为双向LSTM

DATA_DIR = '/content/drive/MyDrive/CS40-2/CS40-2 Data Collections/'
model_name = 'SASI-170'

from torch.utils.data import TensorDataset, DataLoader
from sklearn.model_selection import KFold,StratifiedKFold

results = pd.DataFrame(columns=['fold','precision','recall','f1-score','class 0 f1','class 1 f1','class 2 f1','class 3 f1'])

## 定义train和test, cross_domain的时候交换

from sklearn.model_selection import train_test_split



#X_train, X_test, y_train, y_test = train_test_split(x170, y170, test_size=0.5,random_state=4)

X_train = x3553
y_train = y3553
X_test = x170
y_test = y170
# X_train = x6290
# y_train = y6290
# X_test = x6290
# y_test = y6290


train_data = TensorDataset(X_train, y_train)
test_data = TensorDataset(X_test, y_test)
train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size,drop_last=True)
test_loader = DataLoader(test_data, shuffle=True, batch_size=17,drop_last=True)

model = bilstmAttnMLP(input_size, num_classes, hidden_size, hidden_size2, n_layers, pretrain_embedding, bidirectional).to(device)
optimizer = torch.optim.Adam(model.parameters(), lr=lr)
#criterion = nn.CrossEntropyLoss()
criterion = GamblerLoss()

best_metric = 0.0


for epoch in range(epochs):  
# Set the flag to training
  model.train()
  train_loss = []
  train_correct = 0
  refrain_len = 0.0001
  refrain_incorrect = 0
  train_label_list = []
  pred_label = []
  
  #这里加index 在loss里去掉与predicted去掉refrain的 单独列表refrain
  for data, label in train_loader:
    data = data.to(device)
    label = label.type(torch.LongTensor).to(device)
    # forward + backward + loss + optimize
    optimizer.zero_grad()
    outputs,gain_list = model(data)
    gain = torch.gather(gain_list, dim=1, index=label.unsqueeze(dim=1)).squeeze()

    #print(gain)
    # if gain[i] >= tao, predicted[i] = Refrain or 2 else predicted还是原来那么走
    predicted = torch.argmax(outputs, -1)

    train_label_list.append(label.cpu().numpy().tolist())
    pred_label.append(predicted)
    # implment tao here

    # Custom Loss Function for the Gambler's Loss
    #if len(outputs[gain<tao]) == 0:
    #  continue
    loss = criterion(label,outputs,reward,gain)
    #loss = criterion(outputs, label)

    train_loss.append(loss.item())
    train_correct += torch.sum(predicted[gain<tao] == label[gain<tao])
    refrain_len += len(gain[gain>=tao])
    refrain_incorrect += torch.sum(predicted[gain>=tao] != label[gain>=tao])
    loss.backward()
    optimizer.step()

    del outputs
    torch.cuda.empty_cache()
    '''else:
      loss = criterion(label,predicted,reward,gain)
      #loss = criterion(outputs, label)

      train_loss.append(loss.item())
      train_correct += torch.sum(predicted == label)
      #refrain_len += len(gain[gain>=tao])
      #refrain_incorrect += torch.sum(predicted[gain>=tao] != label[gain>=tao])
      loss.backward()
      optimizer.step()

      del outputs
      torch.cuda.empty_cache()'''

  tem1 = []
  for i in train_label_list:
    for j in i:
      tem1.append(j)
  tem2 = []
  for i in pred_label:
    for j in i:
      tem2.append(j.cpu())
  metrics = 0
  metrics = classification_report(tem1, tem2, labels=[0, 1,2,3], output_dict=True)
  metrics = metrics['macro avg']['f1-score']
  total_loss  = np.mean(train_loss)
  Fail_Safe_Rejects = refrain_incorrect/refrain_len

  print('Epoch: %d, loss: %.5f, macro-f1: %.5f, Fail-Safe_Rejects: %.4f' %(epoch + 1, total_loss,metrics,Fail_Safe_Rejects))
  print('\nTotal num of cov:',str(len(tem2)))
  print('Total num of refrain:',str(len(X_train)-len(tem2)))
  print('\n------------------------------------------------------------------------')
  if metrics > best_metric:
    best_metric = metrics
    best_model_wts = copy.deepcopy(model.state_dict())
  #print(best_metric)
print('Finished Training')

#test

gain_total = []
model.load_state_dict(best_model_wts)
with torch.no_grad():
  test_label_list = []
  pred_label = []
  refrain_len = 0.0001
  refrain_incorrect = 0
  for input_torch,test_label in test_loader:
    
    model.eval()
    input_torch = input_torch.to(device)
    outputs,gain_list = model(input_torch)
    
    for i in gain:
      gain_total.append(i)

    predicted = torch.argmax(outputs, -1)
    gain = torch.gather(gain_list, dim=1, index=predicted.unsqueeze(dim=1)).squeeze()
    label = test_label.to(device)
    
    test_label_list.append(label.cpu().numpy().tolist())
    pred_label.append(predicted)
    refrain_len += len(gain[gain>=tao])
    refrain_incorrect += torch.sum(predicted[gain>=tao] != label[gain>=tao])
    del outputs
    torch.cuda.empty_cache()
    '''
print("Starting testing")
tem1 = []
for i in test_label_list:
  for j in i:
    tem1.append(j)
tem2 = []
for i in pred_label:
  for j in i:
    tem2.append(j.cpu())
Fail_Safe_Rejects = refrain_incorrect/refrain_len
metrics = classification_report(tem1, tem2, labels=[0, 1,2,3], output_dict=True)
#print(metrics)
#results.loc[0] = [0,metrics['macro avg']['precision'],metrics['macro avg']['recall'],metrics['macro avg']['f1-score'],metrics['0']['f1-score'],metrics['1']['f1-score'],metrics['2']['f1-score'],metrics['3']['f1-score']]
#print('Fail-Safe_Rejects: %.6f' %(Fail_Safe_Rejects))
print('cov: %.3f' %(1-refrain_len/len(test_data)))
#results
# results.to_csv(DATA_DIR+f'{model_name}.csv')'''

x = []
index=sorted(range(len(gain_total)), key=lambda k: gain_total[k])
for i in index:
  # 如果是test 3553就把2改成50以上
  if i % 8 == 0:
    x.append(gain_total[i])
tao_list = x
setup_seed(1)
fsr = []
cov = []
f1 = []
for tao in tao_list:
  model.load_state_dict(best_model_wts)
  with torch.no_grad():
    test_label_list = []
    pred_label = []
    refrain_len = 0.0001
    refrain_incorrect = 0
    for input_torch,test_label in test_loader:
      
      model.eval()
      input_torch = input_torch.to(device)
      outputs,gain_list = model(input_torch)
      predicted = torch.argmax(outputs, -1)
      gain = torch.gather(gain_list, dim=1, index=predicted.unsqueeze(dim=1)).squeeze()

      #predicted = torch.argmax(outputs, -1)
      label = test_label.type(torch.LongTensor).to(device)
      test_label_list.append(label[gain<tao].cpu().numpy().tolist())
      pred_label.append(predicted[gain<tao])
      refrain_len += len(gain[gain>=tao])
      refrain_incorrect += torch.sum(predicted[gain>=tao] != label[gain>=tao])
      del outputs
      torch.cuda.empty_cache()
  
  tem1 = []
  for i in test_label_list:
    for j in i:
      tem1.append(j)
  tem2 = []
  for i in pred_label:
    for j in i:
      tem2.append(j.cpu())
  Fail_Safe_Rejects = refrain_incorrect/refrain_len
  metrics = classification_report(tem1, tem2, labels=[0, 1,2,3], output_dict=True)
  fsr.append(Fail_Safe_Rejects.cpu())
  cov.append((1-refrain_len/len(test_data)))
  f1.append(metrics['macro avg']['f1-score'])
  #results.loc[0] = [0,metrics['macro avg']['precision'],metrics['macro avg']['recall'],metrics['macro avg']['f1-score'],metrics['0']['f1-score'],metrics['1']['f1-score']]
  #print('Fail-Safe_Rejects: %.6f' %(Fail_Safe_Rejects),'cov: %.3f' %(1-refrain_len/len(test_data)),'macro-f1: %.3f' %metrics['macro avg']['f1-score'])
  #print('cov: %.3f' %(1-refrain_len/len(test_data)))
  #print(results)
  # results.to_csv(DATA_DIR+f'{model_name}.csv')

index = 0
min = 1
index_cov_85 = 0
for i in cov:
  if (i - 0.85)**2 < min:
    min = (i - 0.85)**2
    index_cov_85 = index
  index += 1
print('85% cov f1 : ',f1[index_cov_85])

